# Chapter 2 Notes Parallel Hardware and Parallel Software## Background### Von Neumann architectureCPU consists of control unit and Arithmetic Logc unitControl unit schedules execution of code, logic unit executes the code.> How does the arithmetic logic unit work?__Registers__ are very fast storgae in the CPU. > How do registers work?__Program counter__ is a special register that stores the address of the next instruction.Front Side bus is the interconnect between CPU and main memory> What is the von Neumann bottleneck?If instructions come from main memory, the throughput of a program is limited by the rate at which data can come over the front side bus into main memory.> What is an operating system?An  operating system is a piece of software that controls access to memory and peripherals, creates, destroys and manages processes.> What is a process composed of?Instructions, memory, metadata, security information, call stack, heap.> What is blocking?A blocking program does not return to the caller until the process has finished execution.A non-blocking program return to the caller immediately.Blocking can obviously be inefficient if the caller could otherwise be better occupied. What kinds of programs benefit from blocking?What if a program needs the information returned by the callee?What about a request to a database? In that case the caller could return to the client empty handed and stall until the called program finishes execution. Otherwise the client is left in limbo while the caller spins in server space."Threads are lighter weight than processes."> What is the difference between a thread and a process?> Can a process be composed of multiple threads? > Can a program be composed of multiple processes under a single thread?Forking, joining.A thread __forks__ from a process when it is created.A thread __joins__ a process when it finishes execution.### Caching basicsL1, L2, L3, SRAM, more expensive, cache hits, cache misses, dirty cache values, prefetching, .> What kind of memory is the Cache composed of? Is  it the same kind as main memory?> What is stored in the cache?> What if two threads are competing for cache and they both request more than a full cache full of data?__locality__ is the principle that memory is executed __near__ other memory, either in _space_ (spatial locality) or in _time_ (temporal locality).> How does the cpu guess what memory is going to be temporally local to other memory?Data comes in __cache blocks__ or __cache lines__. Typically 64 bytes (x16 on a 32 bit system, x8 on a 64 bit system).__cache hits__ and __cache misses____Write-through__ and __write-back__ caches.A __write through__ cache pushes dirty values to main memory as soon as they are written to. A __write back__ cache waits until eviction time to write the dirty values to memory?> What if another thread needs to access the data that has been modified by one thread? If the cache is checked first before every memory call, this would not be a problem.__Associativity__ -- How many addressess in memory can a single cache address represent? All of them? (Fully associative cache). If so, then every single cache line has to be checked at each memory read. Only one? (Direct mapped cache). Then only one cache line has to be checked at any memory read, but > What is the disadvantage of direct mapped caches?"The workings of the CPU cache are controlled by the system hardware."Consequences of locality --This is a greate example -- We expect an array indexing operation in C that is in row major order to be more efficient than a column major indexing operation, because the rows are closer in memory.![experiment results](/Users/levio/Desktop/experiment1.jpg).n-way associativity.## Instruction Level parallelismmultiple processor components (or FUNCTIONAL UNITS) simultaneously executing instructions."instructions executed simultaneously"> For multiple threads? For one thread?> If multiple sets of instructions are being executed simultaneously, how could it be one thread?Approaches:- pipelining -- stagesSuppose we have a workflow with many many operations to perform on data. Suppose we could decompose this workflow into a chain of stages, one pipes into the next. Then we could assign each piece of functionality to a hardware component, routing the output of one intot he input of another. This is more efficient because different instructions can execute simultaneously instead of waiting until it is their turn. If optimizations are possible to steamline the operations in each functional unit,...Not this only makes sense if there are a sequence of regular operations that need to be carried out on a big stack of data that all looks relatively similar.- multiple issue -- multiple instructions launched simultaneously.Multiple threads executing instructions in parallel. Consider accesses to a database. Instead of one thread handling all requests in sequence, let there be two threads that handle each request in parallel. Consider a program that multiplies matrices as part of a data analysis workflow. Suppose that the number of matrices to multiply is quite large. Instead of one program performing the multiplications in sequence, we can halve the time to execute the whole program by evaluating them in parallel. Again, this presupposes a lot of data that looks fairly similar and, but in the case of multiple issue, the instructions are not decomposed into separate steps, but instead considered as a single function that is executed by multiple threads in parallel. > What if we combine the two methods? What if we want to do a data analysis pipeline on many thousands of data sets in parallel? Then for each thread we could set up a pipeline of data analysis functions that feed one into the next. And thus our efficiency is quadrupled.__Static__ multiple issue means the threads are scheduled at compile time.__Dynamic__multiple issue means at runtime.A processor that supports multiple issue is "sometimes said to be"__superscalar_-superscalar means a cpu which is capable of issuing multiple instructions in one clock cycle.Note that a superscalar cpu may be capable of issuing multiple instructions from multiple threads at the same time (SMT).> Superscalar -- think 'like a vector' -- multiple instructions in one clock cycle.> What kinds of instructions can be evaluated in parallel?> What kinds of instructionss can't be evaluated in parallel?Instructions that affect the state of memory executing in parallel is problematic. If load and store instructions are executed on shared registers in parallel, there is an obvious potential for race conditions.Similarly instructions on shared memory.If the memory and registers can be guaranteed to be disjoint, then this problem doesn't exist.> What are the limits of instruction level parallelism?"Average parallelism is about 3 or 4."__speculation__ means inferences that a computer program makes to determine which instructions can be executed in parallel.> What if the speculation is incorrect?> Can we ever know that speculation is certain to be correct?If the speculation turns out to have been incorrect, then the result is discarded and the calculations rerun.> So are the instructions stored in a just-in-case buffer?> How often do speculation misses happen? What if the specaultion misses combined with the inefficiency of storing and rerunning the instructions negates the benefit of doing speculation at all?With instruction level parallelism, we are not talking about multiple threads at all. We are talking about looking at the instructions in a serial program and infering which ones can be run in parallel. Like conditionals and the result of conditionals, assignment operations, function calls, and so forth.Fibonnaci numbers present no possibility for instruction level parallelism. Why? Because prior numbers in the sequence are necessary and sufficient to compute later numbers in the sequence.## Hardware MultithreadingIn hardware multithreading, we introduce the possibility of multiple CPUs executing different routines at once.> Could we introduce hardware level parallelism into the calculation of the fibonnaci numbers?__Thread-level parallelism____coarser grained__ parallelism is composed of larger semantic units -- ie. threads__finer grained__ parallelism is composed of finer elements, eg. instructions.__simultaneous multithreading__ means instructions form multiple threads being executed in one cpu cycle.## Parallel Hardware__Flynn's Taxonomy__ Classifying parallel hardware -- how many simultaneous instruction streams and how many simultabeous data streams can a system handle?__SISD__ -- Single Instruction, Single Data stream. Think of the classical von Neumann architecture. Can execute one instruction at a time, can fetch and store one item of data at a time.__SIMD__ -- Single Instruction, Multiple Data stream. Can execute one instruction ata time, but can fetch or store multiple dta items in parallel. SIMD hardware can apply the same instruction to multiple pieces of data at the same time (vectorization).Single control unit, multiple arithmetic logic units.Think __vector__> What kinds of problems are SIMD systems ideal for? Large sets of data structured like arrays, where the same operations can be carried out in parallel on the data.> What kinds of problems are SIMD systems suboptimal for?Unstructured data, data which require complex and heterogeneous instruction pipelines.__MIMD__ systems -- <mark>Multiple Instruction, Multiple Data</mark> -- multicore, multiple processor cores, each with their own control units and arithmetic logic untis.Think __multicore__.> Do the cores share the same caches?> Do they have their own main memory?> What is their that controls and schedules the communication and the distribution of computing tasks among the cores?> Is their a favoired core, or a central control unit of some kind/__Shared memory models__ vs __distributed memory models__. Their are different implementationsIf memory is shared, the system has the property of __uniform memory access__ (UMA), ie. each core has the same time to access all memory. No memory is private, so no memory is dirty. The disadvantage of this system is that no cpu has fast access to main memory.If memory is distributed, the memory has the property of __nonuniform memory access__ (NUMA).__switches__ control access to a bus. __crossbar____latency__ and __bandwidth__the _latency_ of a sytem is the tiem between when the sender sends a message and the receiver begins to receive it.The _bandwith_ of a system is the rate at which a receiver gets data as soon as they begin to receive it.__false sharing__ is a possible negative consequence of over-eager thread-level parallelism. The system treats the threads as though they are sharing the same memory, even if they are in fact not. Say the memory under consideration is stored on a single cache line. Two threads are operating on it in paraallel, so each time they write to the cache, the cache is invalidated, and the system has to write the cache into memory! > How do we avoid false sharing?> What if we could split up the cache line before the program executes, so that the threads are operating on disjoint cache lines? Big bottleneck in parallel hardware is communication over the interconnect. As more cpus are added, the competition for access to the interconnect grows. Many solutions for this -- switched networks, toroidal meshes and rings in distributed systems, and so on. Which ones are effective?> What is a toroidal mesh?> What is the network bisection and what is it used as a heuristic for?> What is the Flynn taxonomy and what are the principal taxa?> What is the difference between SIMD and MIMD systems?SIMD systemsexecute vectorized instructions, so they execute the same instruction on many data at once.MIMD systems are composed of multiple independent CPU cores, so they are capable of performing many SIMD vectorizations at the same time. In other words, multiple instructions be executed each on many data at the same time.> What kinds of problems would a SIMD solution be better suitd for than a MIMD system?## Modifications to the Von Neumann model## Parallel Hardware## Parallel Software## Input and output## performance## Parallel Program Design