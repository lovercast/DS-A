# Hash Tables

> What is the difference between a direct addressing table and a hash table?
What is a direct addressing table?A direct addressing table is an array of slots corresponding to the keys in the universe U of keys.
Each slot stores an object. we quite simply create an array that is the same size as the universe of keys, assign a key to each element, 
and place each iteem at the slot corresponding to its key.
What if the universe of keys is so large that it cannot reasonably be assigned a matrix in real memory?
Most spaces of numebrs have this property. The integers, for example, or suppose your elements correspond to the number of subsets of an
n-element set. Then the number of keys needed is $\sum_k \binom{n}{k}$, which for sufficiently large n is unreasonable.
> What are the cons of direct addressing?
Cannot accomodate large universes of keys. What is the univers of keys? How do you pick a universe of keys for your data structure?

## Hash Tables to the Rescue

What if the sets of keys is small enough to be stored in memory, buyt the universe of keys is not even close to storable?

__Hash table storage requirements__ $\Theta(|K|)$, while maintaining that search, insertion and deletion take $O(1)$

In direct addressing, the worst-case insertion, search and deletion operations take $O(1)$ time. In hash tables, only average case.

> What are the conditions for hash tables performing at average case, and what are the conditions for ahsht atbles performing at worst case times?

__Hash function__ generates the key for any given element.
__Definition of a hash table__ a hash table is an array corresponding to the image of a hash function on a universe of keys.
ie. A hash table is a set ${1,\cdots, m-1}$, that correspond to the outputs of a hash function $h : U \rightarrow {1, \cdots, m-1}$.
> What's an example of a hash function? What if a hash function is very secure and in theory provides a good average case complexity for the application at hand, but the time complexity of the function itself is below average for hashing functions? Are there these kinds of trade-offs in hashing

Models of hashing -- Open addressing -- Hash an object to a slot in memory, then if there is a collision, proceed along the array until you find an open slot.
Average case constant time, worst case, when the hash table is loaded, linear time, which is considerably worse even than log time.
> What if a hash function were non deterministic? ie. the outputs of a hash function could not be consistently predicted from the inputs. Then there would be no way to find the values that were stored in the hash table if the keys were not known, forcing search to be linear time.

Methods for dealing with collisions. __Chaining__, __Open Addressing__

Suppose that any given element is equally likely to hash to any of the slots.
This is called _-simple uniform hashing__ 
> ie. The hashing obeys a uniform distribution.
In a chained hash table, let us denote the length of the list at index $j$ by $n_j$.
Then the __expected value__ of $n_j$ is $E[n_j] = \alpha = n/m.$

Assume taht the hash function runs in constant time. So then what does the time to search for an element depend on?
"The time required to find an element with key $k$ depends linearly on the length $n_{h(k)}$ of the list $T[h(k)]$."
> Recall that $h(k)$ is  the index into the list where the where the value is stored.
ie. If the list of values at index $h(k)$ is not $1$, then a linear search is required.
If $m << n$, then the likelihood of collision, and hence, linear time searching, is low.

$$
\begin{theorem}
In a hash table in which collisions are resolved by chaining, an unsuccessful search takes average-case time $\Theta(1+\alpha)$,
undert the assumption of simple uniform hashing.
\end{theorem}
$$

> Notice it says unsuccessful search. Why does it specify unsuccessful search? Why not any search?An unsuccessful search will take exactly as long as $\alpha$, while a successful search will take less time on average. We are attempting to reason about the worst -case running time of the algorithm.

$$
\begin{proof}
Say we are searching for an element with key $k$. We begin by hashing $k$, which takes O(1) by assumption, and \Omega(1) by definition.
Furthermore, The length of the list of values at $h(k)$ is $alpha$ by definition, and by assumption the search is unsuccessful, so our search 
proceeds over all $\alpha$ entries until it finds a value with no successor, ie. at least $\alpha$ visits to list nodes, and at most $c \alpha$ for some
constant $c$, hence, $\Theta(1+\alpha)$.
\end{proof}
$$

Under the assumption of simple uniform hashing, any key $k$ not already stored in the table is equally likely to hash to any of the m slots.
The expected time to search unsuccessfully for a key $k$ is the expected time to search to the end of list $T[h(k)]$, which has expected length $E[n_{h(k)}] = \alpha$.
Thus the expected number of elements examined in an unsuccessful search is $\alpha$, and the total time required (including the time for computing $h(k)$) is $\Theta(1+\alpha)$.
> Notes -- "Under the assumption of uniform hashing." What if there weren't uniform hashing?
> Why do you need ot assume unifomr hashing to prove this? 
> recall $\alpha = n/m$ is the expected number of elements at a given list location in the table undert the assumption of uniform hashing.
If there's no uniform hashing, then we can't assume that the expected number of elements at each slot is $\alpha$.
> What is the length of time that it takes for an unsuccessful search? The length of an unsuccessful search is the number of elements in the list $n_{h(k)}$. 
> How do you prove the \Theta?

if $\alpha < 1$, then $O(1+\alpha) < O(2) = O(1).$

What about the case of a succesful search? The probability that a list is searched is proportional to the length of the list. "Nonetheless, the expected search time
still turns out to be \Theta(1+\alpha).

__Theorem__ 11.2
"In a hash table in which collisions are solved by chaining, a successful search 
takes average-case time \Theta(1+\alpha), under the assumption of simple uniform hashing."

__Proof__ Under the assumption of uniform hashing, any element is equally likely to slot at any given entry in a hash table. In the case of a successful searcy, the number of nodes visited is the 
number of nodes in the list at $h(k)$ before the search for element. Assuming the searched for element has no relationship to the order that the elements were entered into the table,
then the expected position of the searched for element in the list is $\alpha/2$. Hence, the time to find this element including computing the hash function is $\Theta(1+\alpha/2) = \Theta(1+\alpha)$.

"We assume that the searched for element is equally likely to be any of the elements in the table."


